{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import pyflux as pf\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "splitDataPath = '../dataProcessing/splitData/splitData.csv'\n",
    "testDataPath = '../dataProcessing/splitData/testData.csv'\n",
    "splitLanePath = '../dataProcessing/splitByLaneData/'\n",
    "testSplitLanePath = '../dataProcessing/testSplitByLaneData/'\n",
    "testWeatherPath = '../dataSets/testing_phase1/weather (table 7)_test1.csv'\n",
    "table3 = '../dataSets/training/links (table 3).csv'\n",
    "table4 = '../dataSets/training/routes (table 4).csv'\n",
    "table7 = '../dataProcessing/splitData/weather.csv'\n",
    "arimaPicturePath = '../dataProcessing/arimaPicture/'\n",
    "arimaPredict = '../dataProcessing/arimaPredict1/'\n",
    "\n",
    "def getData(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data\n",
    "\n",
    "def regularTime(s):\n",
    "    time = datetime.strptime(s,\"%Y-%m-%d %H:%M:%S\")\n",
    "    minute = time.minute\n",
    "    time = time.replace(minute = (minute/20)*20,second = 0)\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def splitDataByLane(data,lanes,path):\n",
    "#     label = ['intersection_id','tollgate_id','lane_id','starting_time','time_window','week','pressure','sea_pressure','wind_direction','wind_speed','temperature','rel_humidity','precipitation','travel_lane','travel_velocity','travel_length']\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        return\n",
    "    label = ['lane_id','time','pressure','sea_pressure','wind_direction','wind_speed','temperature','rel_humidity','precipitation','travel_velocity']\n",
    "    dataSet = data.copy()\n",
    "    dataSet['travel_velocity'] = dataSet['travel_velocity'].apply(lambda x: np.array(x.split(';')).astype(float))\n",
    "    dataSet['travel_lane'] = dataSet['travel_lane'].apply(lambda x: np.array(x.split(';')).astype(float))\n",
    "    dataSet['travel_length'] = dataSet['travel_length'].apply(lambda x: np.array(x.split(';')).astype(float))\n",
    "    dataSet['travel_seq'] = dataSet['travel_seq'].apply(lambda x: x.split(';'))\n",
    "    dataSet['starting_time'] = dataSet['starting_time'].apply(regularTime)\n",
    "    dataSet.fillna(-1)\n",
    "    \n",
    "    dataSet.drop(['vehicle_id','total_velocity','total_length'],axis = 1,inplace = True)\n",
    "    \n",
    "    \n",
    "    weather = []\n",
    "    weaTmp = dataSet[['starting_time','pressure','sea_pressure','wind_direction','wind_speed','temperature','rel_humidity','precipitation']]\n",
    "    for name,group in weaTmp.groupby(['starting_time']):\n",
    "        temp = []\n",
    "        temp.extend(group.values[0].tolist())\n",
    "        weather.append(temp)\n",
    "    wLabel = ['time','pressure','sea_pressure','wind_direction','wind_speed','temperature','rel_humidity','precipitation']\n",
    "    weather = pd.DataFrame(weather,columns = wLabel)\n",
    "    \n",
    "    print '==========================='\n",
    "    print 'weather data is'\n",
    "    print weather.info()\n",
    "    \n",
    "    result = pd.concat(list(dataSet['travel_seq'].apply(lambda x: pd.DataFrame(x,columns=['lane_id']))))\n",
    "    result['time'] = result['lane_id'].apply(lambda x: x.split('#')[1])\n",
    "    result['time'] = result['time'].apply(regularTime)\n",
    "    result['lane_id'] = result['lane_id'].apply(lambda x: x.split('#')[0])\n",
    "    result['travel_velocity'] = pd.concat(list(dataSet['travel_velocity'].apply(lambda x: pd.DataFrame(x))))\n",
    "    result = result.groupby(['time','lane_id']).mean().reset_index()\n",
    "    result = pd.merge(result,weather,how = 'left',on = ['time'])\n",
    "    \n",
    "    result['time'] = result['time'].apply(pd.to_datetime)\n",
    "    \n",
    "    print '==========================='\n",
    "    print 'begain to split'\n",
    "    print result.info()\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    for lane in lanes:\n",
    "        temp = result[:][result['lane_id'] == lane]\n",
    "        temp.drop(['lane_id'],axis = 1).to_csv(path+'lane_'+lane+'.csv')\n",
    "#       print '==========='\n",
    "#       print lane\n",
    "#       print dataByLink[lane].info()\n",
    "    \n",
    "    \n",
    "\n",
    "def handleWeaInflu(data,isTest = False):\n",
    "    label = ['travel_velocity']\n",
    "    # some day (9-28,10-10) lack the information of weather\n",
    "    handleData = data.copy()\n",
    "\n",
    "    handleData = handleData['travel_velocity'][handleData['travel_velocity']<30].to_frame()\n",
    "    \n",
    "    # ga\n",
    "    if not isTest:\n",
    "        time=pd.DataFrame(pd.date_range(start = '20160719000000',end = '20161017234000',freq = '20min'),columns=['time'])\n",
    "    else:\n",
    "        temp1 = pd.DataFrame(pd.date_range(start = '20161018060000',end = '20161018074000',freq = '20min'),columns=['time'])\n",
    "        temp2 = pd.DataFrame(pd.date_range(start = '20161018150000',end = '20161018164000',freq = '20min'),columns=['time'])\n",
    "        times = []\n",
    "        times.append(temp1.copy())\n",
    "        times.append(temp2.copy())\n",
    "        for i in range(6):\n",
    "            temp1['time'] = temp1['time'] + pd.DateOffset(days = 1)\n",
    "            temp2['time'] = temp2['time'] + pd.DateOffset(days = 1)\n",
    "            times.append(temp1.copy())\n",
    "            times.append(temp2.copy())\n",
    "        \n",
    "        time = pd.concat(times)\n",
    "        time.reset_index()\n",
    "        \n",
    "        \n",
    "    result = pd.merge(time,handleData,how='left',left_on='time',right_index=True)\n",
    "    result.set_index('time',inplace = True)\n",
    "    result = result.interpolate()\n",
    "    result.fillna(method = 'pad')\n",
    "\n",
    "    return result\n",
    "\n",
    "# def recoverWea(data):\n",
    "    \n",
    "    \n",
    "def predict():\n",
    "    routes = {}\n",
    "    links = {}\n",
    "    lanes = []\n",
    "    with open(table3,'r') as fr:\n",
    "        lines = csv.reader(fr)\n",
    "        for line in lines:\n",
    "            if lines.line_num == 1:\n",
    "                continue\n",
    "            # length,lane\n",
    "            links[line[0]] = [int(line[1]),int(line[3])]\n",
    "            lanes.append(line[0])\n",
    "\n",
    "    with open(table4,'r') as fr:\n",
    "        lines = csv.reader(fr)\n",
    "        for line in lines:\n",
    "            if lines.line_num == 1:\n",
    "                continue\n",
    "            if line[0] not in routes.keys():\n",
    "                routes[line[0]] = {}\n",
    "            routes[line[0]][line[1]] = line[2].split(',')\n",
    "            \n",
    "            \n",
    "    print '==========================='\n",
    "    print 'preparation completes'\n",
    "    \n",
    "    trainData = getData(splitDataPath)\n",
    "    testData = getData(testDataPath)\n",
    "    splitDataByLane(trainData,lanes,splitLanePath)\n",
    "    splitDataByLane(testData,lanes,testSplitLanePath)\n",
    "    predictLaneVelo = {}\n",
    "    \n",
    "    \n",
    "    print '==========================='\n",
    "    print 'prepate with lane'\n",
    "    if not os.path.exists(arimaPicturePath):\n",
    "        os.makedirs(arimaPicturePath)\n",
    "    if not os.path.exists(arimaPredict):\n",
    "        os.makedirs(arimaPredict)\n",
    "    \n",
    "#     for lane in lanes:\n",
    "    for lane in range(110,124):\n",
    "        print '==========================='\n",
    "        print lane\n",
    "        data = pd.read_csv(splitLanePath+'lane_'+lane+'.csv')\n",
    "        data['time'] = data['time'].apply(pd.to_datetime)\n",
    "        data.set_index('time',inplace = True)\n",
    "        testData= pd.read_csv(testSplitLanePath+'lane_'+lane+'.csv')\n",
    "        testData['time'] = testData['time'].apply(pd.to_datetime)\n",
    "        testData.set_index('time',inplace = True)\n",
    "        \n",
    "\n",
    "        testData = handleWeaInflu(testData,True)\n",
    "        handleData = handleWeaInflu(data)\n",
    "#         handleData.plot(figsize = (100,10))\n",
    "        result = []\n",
    "#         plt.savefig(arimaPicturePath+lane+'_trace_picture.png')\n",
    "#         plot_acf(handleData).savefig(arimaPicturePath+lane+'_acf_picture.png')\n",
    "#         plot_pacf(handleData).savefig(arimaPicturePath+lane+'_pacf_picture.png')\n",
    "\n",
    "        i = 0\n",
    "        size = 14\n",
    "        while(i<size):\n",
    "\n",
    "            if i%2!=0:\n",
    "                print handleData.tail()\n",
    "                predict = run_aram(handleData,6,6,21)\n",
    "                handleData = pd.concat([handleData,predict,testData[i*6:(i+1)*6]])\n",
    "                \n",
    "                result.append(predict[0:6])\n",
    "                print handleData.tail()\n",
    "                predict = run_aram(handleData,6,6,21)\n",
    "                handleData = pd.concat([handleData,predict])\n",
    "                result.append(predict[0:6])\n",
    "            else:\n",
    "                print handleData.tail()\n",
    "                predict = run_aram(handleData,6,6,18)\n",
    "                handleData = pd.concat([handleData,predict,testData[i*6:(i+1)*6]])\n",
    "            \n",
    "            i = i+1\n",
    "        temp = pd.concat(result)\n",
    "        temp.to_csv(arimaPredict+'lane_'+lane+'.csv')\n",
    "\n",
    "        \n",
    "    \n",
    "    for lane in links.keys():\n",
    "        predictLaneVelo[lane] = pd.read_csv(arimaPredict+'lane_'+lane+'.csv')\n",
    "        # recover weather\n",
    "        predictLaneVelo[lane]['travel_time'] = float(links[lane][0])/predictLaneVelo[lane]['travel_velocity']\n",
    "    \n",
    "    result = []\n",
    "    for intersection in routes.keys():\n",
    "        for tollgate in routes[intersection].keys():\n",
    "            lane = routes[intersection][tollgate][0]\n",
    "            temp = predictLaneVelo[lane].copy()\n",
    "            temp['intersection'] = intersection\n",
    "            temp['tollgate'] = tollgate\n",
    "            temp.drop(['travel_time','travel_velocity'],axis = 1,inplace = True)\n",
    "            temp['time_window'] = temp.index\n",
    "            temp['time_window'] = temp['time_window'].apply(lambda x: '['+str(x)+','+str(pd.to_datetime(x)+pd.DateOffset(minutes = 20))+')')\n",
    "            temp.drop(['time'],axis =1 ,inplace = True)\n",
    "            temp['avg_travel'] = 0\n",
    "            for lane in routes[intersection][tollgate]:\n",
    "                temp['avg_travel_time'] = temp['avg_travel_time']+predictLaneVelo[lane]['travel_time']\n",
    "            result.append(temp)\n",
    "    \n",
    "\n",
    "    result = pd.concat(result)    \n",
    "    result.to_csv(arimaPicturePath+'result.csv')\n",
    "    \n",
    "        \n",
    "\n",
    "def test_stationarity(timeseries):\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    print dftest\n",
    "    return dftest[1]\n",
    "\n",
    "def bestDiff(df, maxdiff = 8):\n",
    "    p_set = {}\n",
    "    for i in range(0, maxdiff):\n",
    "        temp = df.copy() #每次循环前，重置\n",
    "        if i == 0:\n",
    "            temp['diff'] = temp[temp.columns[1]]\n",
    "        else:\n",
    "            temp['diff'] = temp[temp.columns[1]].diff(i)\n",
    "            temp = temp.drop(temp.iloc[:i].index) #差分后，前几行的数据会变成nan，所以删掉\n",
    "        pvalue = test_stationarity(temp['diff'])\n",
    "        p_set[i] = pvalue\n",
    "        p_df = pd.DataFrame.from_dict(p_set, orient=\"index\")\n",
    "        p_df.columns = ['p_value']\n",
    "    i = 0\n",
    "    while i < len(p_df):\n",
    "        if p_df['p_value'][i]<0.01:\n",
    "            bestdiff = i\n",
    "            break\n",
    "        i += 1\n",
    "    return bestdiff\n",
    "\n",
    "def produce_diffed_timeseries(df, diffn):\n",
    "    if diffn != 0:\n",
    "        df['diff'] = df[df.columns[1]].apply(lambda x:float(x)).diff(diffn)\n",
    "    else:\n",
    "        df['diff'] = df[df.columns[1]].apply(lambda x:float(x))\n",
    "    df.dropna(inplace=True) #差分之后的nan去掉\n",
    "    return df\n",
    "\n",
    "def choose_order(ts, maxar, maxma):\n",
    "    print 'choose order'\n",
    "    order = sm.tsa.arma_order_select_ic(ts, maxar, maxma, ic=['aic', 'bic', 'hqic'])\n",
    "    print 'finish'\n",
    "    return order.bic_min_order\n",
    "\n",
    "def predict_recover(ts, df, diffn):\n",
    "    if diffn != 0:\n",
    "        ts.iloc[0] = ts.iloc[0]+df['log'][-diffn]\n",
    "        ts = ts.cumsum()\n",
    "    ts = np.exp(ts)\n",
    "#    ts.dropna(inplace=True)\n",
    "    print('还原完成')\n",
    "    return ts\n",
    "\n",
    "\n",
    "def run_aram(df, maxar, maxma, test_size):\n",
    "    \n",
    "    train = df.dropna()\n",
    "    train['log'] = np.log(train[train.columns[0]])\n",
    "    diffn = 0\n",
    "    train = produce_diffed_timeseries(train, 1)\n",
    "    if test_stationarity(train[train.columns[2]]) < 0.01:\n",
    "        print('平稳')\n",
    "    else:\n",
    "        diffn = best_diff(train, maxdiff = 8)\n",
    "        train = produce_diffed_timeseries(train, diffn)\n",
    "        print('差分阶数为'+str(diffn)+'，已完成差分')\n",
    "    print('开始进行ARMA拟合')\n",
    "    diffn = diffn + 1\n",
    "    order = choose_order(train[train.columns[2]], maxar, maxma)\n",
    "    print('模型的阶数为：'+str(order))\n",
    "    _ar = order[0]\n",
    "    _ma = order[1]\n",
    "    model = pf.ARIMA(data=train, ar=_ar, ma=_ma, integ=diffn,target='diff',family=pf.Normal())\n",
    "    x = model.fit(\"MLE\")\n",
    "    x.summary()\n",
    "    test_predict = model.predict(test_size)\n",
    "    test_predict.columns = ['travel_velocity']\n",
    "    test_predict.index = test_predict.index + pd.DateOffset(minutes = test_size*20)\n",
    "    test_predict = predict_recover(test_predict, train, diffn)\n",
    "    print test_predict.head()\n",
    "    return test_predict\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "preparation completes\n",
      "===========================\n",
      "prepate with lane\n",
      "===========================\n",
      "100\n",
      "                     travel_velocity\n",
      "time                                \n",
      "2016-10-17 22:20:00         8.686026\n",
      "2016-10-17 22:40:00        13.242009\n",
      "2016-10-17 23:00:00         9.430894\n",
      "2016-10-17 23:20:00         5.852252\n",
      "2016-10-17 23:40:00         5.852252\n",
      "(-21.566579247982968, 0.0, 35, 6515, {'5%': -2.8619837376559043, '1%': -3.4313541256158335, '10%': -2.5670061981824812}, 1152.4247191280629)\n",
      "平稳\n",
      "开始进行ARMA拟合\n",
      "choose order\n",
      "finish\n",
      "模型的阶数为：(2, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The computed initial MA coefficients are not invertible\nYou should induce invertibility, choose a different model order, or you can\npass your own start_params.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-19be86c8c029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-3d8fbc679f23>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0mhandleData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_aram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandleData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0mhandleData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandleData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3d8fbc679f23>\u001b[0m in \u001b[0;36mrun_aram\u001b[0;34m(df, maxar, maxma, test_size)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0m_ar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0m_ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARIMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6552\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/statsmodels/tsa/arima_model.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, trend, method, transparams, solver, maxiter, full_output, disp, callback, start_ar_lags, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                                            \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                                            \u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m                                            callback, start_ar_lags, **kwargs)\n\u001b[0m\u001b[1;32m   1152\u001b[0m         \u001b[0mnormalized_cov_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m  \u001b[0;31m# TODO: fix this?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         arima_fit = ARIMAResults(self, mlefit._results.params,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/statsmodels/tsa/arima_model.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, trend, method, transparams, solver, maxiter, full_output, disp, callback, start_ar_lags, **kwargs)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# estimate starting parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             start_params = self._fit_start_params((k_ar, k_ma, k), method,\n\u001b[0;32m--> 956\u001b[0;31m                                                   start_ar_lags)\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparams\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# transform initial parameters to ensure invertibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/statsmodels/tsa/arima_model.pyc\u001b[0m in \u001b[0;36m_fit_start_params\u001b[0;34m(self, order, method, start_ar_lags)\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglike_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;31m#start_params = [.1]*(k_ar+k_ma+k_exog) # different one for k?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mstart_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_start_params_hr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_ar_lags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mstart_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invtransparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/statsmodels/tsa/arima_model.pyc\u001b[0m in \u001b[0;36m_fit_start_params_hr\u001b[0;34m(self, order, start_ar_lags)\u001b[0m\n\u001b[1;32m    562\u001b[0m         elif q and not np.all(np.abs(np.roots(np.r_[1, start_params[k + p:]]\n\u001b[1;32m    563\u001b[0m                                               )) < 1):\n\u001b[0;32m--> 564\u001b[0;31m             raise ValueError(\"The computed initial MA coefficients are not \"\n\u001b[0m\u001b[1;32m    565\u001b[0m                              \u001b[0;34m\"invertible\\nYou should induce invertibility, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                              \u001b[0;34m\"choose a different model order, or you can\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The computed initial MA coefficients are not invertible\nYou should induce invertibility, choose a different model order, or you can\npass your own start_params."
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
